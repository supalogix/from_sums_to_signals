\section{Amari: Geometry Learns to Think}

\subsection{Shun-ichi Amari: Curvature in the Space of Belief}

If Jeffreys gave us belief as integration, and Jordan gave us belief as optimization, then Shun-ichi Amari gave us belief as \emph{geometry}.

Where Michael Jordan treated KL divergence as a functional to minimize, Amari treated it as a kind of \emph{distance}—but not in the usual Euclidean sense. Instead, he showed that KL divergence defines a local curvature on the space of probability distributions, encoded by the Fisher Information metric. In this geometric view, learning becomes motion: belief updates trace geodesics across a curved manifold of possible models.

This is the heart of \textbf{information geometry}—a field Amari pioneered—where statistical inference is governed by differential geometry. Gradients become \emph{natural gradients}, adjusted for the curvature of the belief space, allowing more efficient and principled learning updates.

\[
I(\theta) = \int f(x \mid \theta) \left( \frac{\partial \log f(x \mid \theta)}{\partial \theta} \right)^2 dx
\]

This same Fisher Information that Jeffreys used to define his prior, and that Jordan used to characterize variational families, becomes in Amari’s hands a full-blown \textbf{Riemannian metric}. Geometry and learning are no longer separate. They are one and the same.


\begin{tcolorbox}[colback=yellow!5!white, colframe=yellow!50!black, title=Ernst Mach and the Sensory Roots of Inference]
    Before Bayesian inference found its natural gradient, it found its natural habitat—in the physiology of perception.
    
    \textbf{Ernst Mach}, a 19th-century physicist-philosopher, argued that the laws of physics (and thought itself) arise not from some external metaphysics, but from the way we experience the world through sensation. He believed cognition was fundamentally shaped by regularities in sensory input, not by abstract reason.
    
    This idea—that inference is grounded in perception—foreshadowed modern models of learning as \emph{adaptive geometry}. Mach’s emphasis on sensory experience as the bedrock of knowledge resonates deeply with \textbf{Shun-ichi Amari’s} later work in information geometry, where learning systems adapt not just to data, but to the curvature of the belief space they inhabit.
    
    In a sense, Mach replaced Newton’s laws with perceptual laws.  
    Amari did the same—replacing logic with landscape.
\end{tcolorbox}


\subsection{From Mutual Dependence to Expected Surprise}

Originally defined as a double sum, mutual information too became an expectation over divergence:

\[
I(X; Y) = \int p(y) \left[ \int p(x \mid y) \log \frac{p(x \mid y)}{p(x)} \, dx \right] dy
\]

This nested integral has no closed form in most real-world applications. Computing it is hard—but its interpretation is beautiful: it is the average surprise you’d feel about \( X \) after learning \( Y \).

Mutual information is what powers modern learning to ask not “what’s true?” but “what’s informative?”

\subsection{From Precision to Principle: Fisher Information Reused}

Originally introduced by Ronald Fisher as a bound on estimator variance, Fisher Information quantified how sensitive a likelihood function was to changes in its parameters. It told you how much the data “sharpens” your estimate—how much precision you gain from observation. In this form, it entered Jeffreys’ thinking as a way to define a prior that was invariant under reparametrization:

\[
\pi(\theta) \propto \sqrt{I(\theta)} \quad \text{with} \quad I(\theta) = \int f(x \mid \theta) \left( \frac{\partial \log f(x \mid \theta)}{\partial \theta} \right)^2 dx
\]

But Shun-ichi Amari saw something deeper in this formula.

To Amari, the Fisher Information wasn’t just a measure of precision—it was the local curvature of belief space. It defined a Riemannian metric on the manifold of probability distributions, turning statistical inference into a problem of geometry. This reinterpretation gave birth to \textbf{information geometry}.

In this framework, nearby distributions are “close” if their KL divergence is small. And just as Euclidean geometry measures distance with a dot product, information geometry measures divergence with Fisher Information:

\[
D_{\mathrm{KL}}(p(\theta + d\theta) \parallel p(\theta)) \approx \frac{1}{2} d\theta^T I(\theta) d\theta
\]

This second-order approximation shows that Fisher Information plays the same role as a metric tensor—it defines the inner product on the tangent space of probability distributions.

Inference, in Amari’s hands, becomes movement through curved space. Ordinary gradients are replaced with \textbf{natural gradients} that follow the geodesics of this space, adjusting not just for slope but for shape. Learning becomes a kind of navigation—one that honors the geometry of uncertainty.

What began as a technical tool for bounding error becomes, in this light, a unifying principle:  
Fisher Information doesn’t just measure precision—it defines the structure of belief itself.
