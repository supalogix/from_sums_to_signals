\section{Emergent Structure in Deep Learning: A Pragmatist Interpretation}

\subsection{Wittgenstein’s Later Turn: From Logic to Use}

In his early work, \textit{Tractatus Logico-Philosophicus}, \textbf{Ludwig Wittgenstein} sought to describe the world through logical propositions that mirrored reality. But in his later work, especially \textit{Philosophical Investigations}, he radically changed course. Meaning, he argued, is not a matter of internal correspondence to some abstract essence—it is a function of \emph{use}.

This gave rise to his famous concept of \textbf{language games}: the idea that the meaning of a word arises from the context in which it is used, embedded in a form of life, not from some intrinsic or symbolic mapping.

\begin{quote}
    ``For a large class of cases—though not for all—in which we employ the word 'meaning' it can be defined thus: the meaning of a word is its use in the language.''\\
    — \textit{Philosophical Investigations} \S43
\end{quote}

This shift from definition to use, from symbol to social behavior, echoes in modern deep learning. Neural networks do not learn meanings in any ontological sense. They learn \emph{patterns of use}—statistical regularities across massive corpora of human activity.

\subsection{From Symbolic Representations to Emergent Concepts}

Traditional approaches to AI treated knowledge as a system of symbols and rules. Concepts were handcrafted, encoded explicitly, and manipulated via logic. But deep learning abandoned this model. Instead of programming knowledge, we let the model \emph{discover} it.

A neural network is not given an explicit structure for ``cat,'' ``strategy,'' or ``trust.'' It forms distributed representations—vectors in high-dimensional space—that acquire functional meaning by how they behave in the network’s training regime.

These internal representations are not interpretable in the classical sense. There is no fixed symbol-to-meaning map. Instead, their ``meaning'' is determined pragmatically: through the transformations they enable, the predictions they improve, and the losses they minimize.

\subsection{Implementation: How Emergence Happens}

In practice, emergent structure arises from the interaction of three core elements:

\begin{itemize}
    \item \textbf{Architecture}: Layered, compositional systems (e.g., transformers, CNNs) allow abstraction to emerge hierarchically.
    \item \textbf{Objective Function}: The model’s ``game''—what it’s trying to win. This includes losses like cross-entropy, contrastive learning objectives, or even reward functions in reinforcement learning.
    \item \textbf{Optimization Dynamics}: Gradient descent and stochastic updates iteratively sculpt the weight space to encode functional structure.
\end{itemize}

These components jointly define an \textbf{information ecology} where structure is not imposed from above but \emph{discovered through use}. Features like part-whole hierarchies, syntactic trees, or causal variables may emerge—not because the network is told to look for them, but because those structures are statistically useful for minimizing error.

\subsection{Wittgenstein and the Neural Epistemology}

Just as Wittgenstein denied the possibility of a ``private language'' with no social grounding, we may ask: does a neural network's internal representation have meaning if it cannot be interpreted?

Perhaps not in the symbolic sense. But in the pragmatist sense—does it work? Does it generalize? Does it let the model win its language game?

Then yes. It means.

\vspace{0.5em}
\begin{quote}
    \emph{In the end, meaning is not given—it is earned.}
\end{quote}
