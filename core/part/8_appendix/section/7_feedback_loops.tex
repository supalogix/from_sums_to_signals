\subsection{Feedback Loops: Because the Model Thought All Users Were Bots for Two Months}

\textbf{Act I: The Spiral Begins}

At a midsize fintech company, a fraud detection model began quietly flagging nearly every new user as suspicious.

Conversion rates dropped.\\
Support tickets piled up.\\
Engineering blamed data science.\\
Data science blamed data engineering.\\
Marketing just said, ``Something feels off.''

But technically, the model was doing exactly what it was trained to do.

\vspace{1em}
\textbf{Act II: The Innocent Mistake}

One week, a few high-risk accounts slipped through.\\
They weren’t caught in time.

So the feedback loop retrained the model using those users as negative examples.\\
The model learned: ``These are bad.''

Then it flagged more borderline users.\\
Those too were fed back in as negative examples.

The loop tightened. The model hardened.

\vspace{1em}
\textbf{Act III: The Botpocalypse}

Eventually, the model reached its final form:  
Everyone was a bot.

Especially if you had a middle name.\\
Or used a VPN.\\
Or signed up during lunch.

The model wasn’t broken.\\
It was confidently, aggressively wrong.

\vspace{1em}
\textbf{Act IV: The Blame Game}

No one noticed at first.\\
By the time they did, it had been two months.

Support was overwhelmed. Conversions had cratered.\\
And no one could explain how it had gotten so bad — so fast.

The feedback loop had done its job. Just without supervision.

\vspace{1em}
\textbf{Epilogue: Why Feedback Loops Matter}

Without oversight — human or otherwise — your model can spiral into a feedback chamber of its own mistakes.

Label drift, concept drift, and unintended self-reinforcement are real.\\
Your model doesn’t know what it doesn’t know.

If you don’t close the loop with careful monitoring, \textbf{your system will confidently and automatically learn the wrong thing.}

