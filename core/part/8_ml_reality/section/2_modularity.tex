\section{Modularity: Because the Intern Deployed a Hardcoded Model to Prod}

\subsection{Act I: The Intern and the Ancient Logs}

It always starts the same way.

An intern joins a big company: the kind with seventeen internal platforms named after birds, seven ways to deploy code, and exactly zero onboarding documentation that's less than two fiscal years out of date.

Their mission? Train a “quick linear model” for a low-risk, low-visibility feature.

You know the type: technically in production, but nobody cares unless it’s broken. It’s the kind of feature that lives in the shadowy basement of the product stack: somewhere between “critical to compliance” and “nobody’s sure who owns this anymore.” 

It doesn’t drive revenue. It doesn’t get showcased at all-hands. It’s the ML equivalent of changing the default font size in legal disclaimers. The only time it gets attention is when it silently fails, and suddenly a VP is asking why the "numbers feel off" in a dashboard no one has looked at since the last reorg.

And yet --- paradoxically --- this model has to be robust, fast, explainable, and deployable by Friday. It’s the Schrödinger’s cat of machine learning: simultaneously irrelevant and vital, invisible and on fire.


Now here's the catch: No one could tell them where the data came from.  Or where it went.  Or what shape it was in.  Or if it even existed outside of a stale JIRA ticket and someone’s memory of a meeting from last October.

\begin{quote}
In other words: institutional memory had evaporated.
\end{quote}

What was once a functioning data pipeline was now a digital ruin — still running, but no one dared touch it. The current team treated it like an ancient relic from a lost civilization. No documentation, no dashboards, no tests. Just a handful of cryptic cron jobs duct-taped together with bash scripts and hope.

Trying to debug it felt like decoding the Dead Sea Scrolls — except half the scrolls are in YAML, and the other half are missing entirely. Ask around, and you’d hear whispers of “someone in infra might remember the original config,” or “maybe try grepping old Git commits from 2019?”

You don’t maintain this kind of system. You pray it doesn’t break during daylight hours — because if it does, it’ll summon a war room full of confused engineers, each discovering in real time that they are now the de facto owner.

So the intern did what anyone would do under the circumstances:

\begin{itemize}
	\item They grep’ed.
	\item They found an old log folder with timestamps from the pre-pandemic era.
	\item And they wrote \texttt{pandas.read\_csv()} like it was a sacred ritual passed down from Stack Overflow itself.
\end{itemize}

And miraculously, it ran... sort of.

There were nulls in half the columns, timestamps in three different formats, and a mysterious field called \texttt{v2FlagTemp} that no one ever defined.

Nobody knew what \texttt{v2FlagTemp} actually did. Some said it was a deprecation marker. Others claimed it toggled a legacy feature that hadn’t existed since the Kubernetes migration of ’19. One engineer swore it flipped to \texttt{TRUE} during solar flares.

Meanwhile, the timestamps were a small chaos engine unto themselves — sometimes ISO, sometimes American short date, occasionally slashed Y/M/D, and once, just once, a format that appeared to be Morse code.

Parsing the logs felt less like data engineering and more like digital paleontology. You weren’t cleaning data. No, you were performing forensic analysis on a crime scene left behind by a team that vanished during a global pandemic.

\begin{lstlisting}[
    caption={Sample log entries recovered from the pre-pandemic era},
    label={lst:ancientlogs},
    basicstyle=\ttfamily\small,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    breaklines=true,
    breakatwhitespace=false,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
  ]
  2020-01-03 10:15:32 INFO  | feature1=12.3 feature2=7.1 target=? v2FlagTemp=TRUE
  Jan 03 2020 10:15:33 INFO | feature1=13.1 feature2=8.0 target=4.0
  2020/01/03 10:15:34 INFO  | feature1=nan feature2=6.9 target=?
  2020-01-03 10:15:35 WARN  | v2FlagTemp=FALSE dropped packet from node17
  2020-01-03 10:15:36 INFO  | feature1=11.8 feature2=7.3 target=3.9 v2FlagTemp=1
  ??? 10:15:37 --- | featurX=?? fea2=error targt=missing
  2020-01-03 10:15:38 INFO  | feature1=13.0 feature2=8.1 target=4.1
  \end{lstlisting}
  

But the intern pressed on because a few \texttt{dropna()} calls and some light string parsing would bring order to the chaos.  Hope, after all, is the most dangerous optimizer.

Armed with Stack Overflow tabs and unshakable faith in pandas, he began the sacred rite of modern data cleaning: (a) removing nulls without asking why they were there, (b)coercing all timestamps into ISO format (with silent fallback), and (c) mapping suspicious Boolean-like strings to something vaguely coherent. \texttt{v2FlagTemp}? Treated as a categorical, obviously.  Unused features? Dropped with the righteous confidence of someone who had never seen a postmortem.  \textbf{Because nothing says “production ready” like blindly transforming orphaned telemetry logs from an undocumented cronjob.}

\begin{quote}
By the time the Jupyter notebook was done, the data \textit{looked} ``clean''.  Not correct. Not meaningful. But clean enough to pass a code review; and that, in the intern’s defense, is often good enough to get a green checkmark in a quarterly slide deck.
\end{quote}


\begin{figure}[H]
\centering

% === First row ===
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Engineer 1}
    {Engineer 2}
    {\footnotesize Let's just log it like this for now. I'll document it later.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{Prologue: It begins, as always, with good intentions.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Engineer 1}
    {Engineer 2}
    {\footnotesize v2FlagTemp? Yeah, just set it to TRUE if the system feels unstable.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{The design philosophy: chaos with confidence.}
\end{subfigure}

\vspace{1em}

% === Second row ===
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Executive 1}
    {Executive 2}
    {\footnotesize We’re preparing a strategic reorg. Most of the original team will be gone by Friday.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{The culling: efficiencies must be unlocked.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Executive 1}
    {Executive 2}
    {\footnotesize But I’m not worried. I’ve been assured everything is fully documented.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{The punchline: spoken like someone who’s never used Confluence.}
\end{subfigure}

\caption{The Birth of a Legacy System: High hopes and zero documentation.}
\end{figure}


\subsection{Act II: The Birth of the Monolith}

Fresh off the intoxicating high of successfully reading a CSV the intern embarked on their next great odyssey.

He did what every unsupervised intern with vague requirements, root access, and a deeply misplaced sense of destiny inevitably does: he wrote a script.

Not a pipeline. Not a collection of well-documented, reusable modules.

No. This was different.

It was a single, sprawling, glorious file—an unbroken stream of code that took data from cradle to grave. From loading to preprocessing to training to evaluation to deployment to celebratory print statement. All in one script. All in one breath.

\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{teal},
  breaklines=true,
  breakatwhitespace=false,
  postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space},
  showstringspaces=false,
  frame=single,
  caption={The Monolith Script in All Its Glory},
  label={lst:monolith},
  numbers=left,
  numberstyle=\tiny,
  language=Python
}

\begin{lstlisting}[language=Python]
# the_monolith.py

import os
import re
import pickle
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Step 1: Parse logs (from a hardcoded path)
log_path = "/mnt/interns_box_of_mystery/logs/training_data.log"
with open(log_path, "r") as file:
    lines = file.readlines()

# Step 2: Extract data using regex and vibes
data = []
for line in lines:
    match = re.match(r".*INFO\s+\|\s+feature1=(\d+\.\d+)\s+feature2=(\d+\.\d+)\s+target=(\d+\.\d+)", line)
    if match:
        data.append(tuple(map(float, match.groups())))

df = pd.DataFrame(data, columns=["feature1", "feature2", "target"])

# Step 3: Clean data using chained one-liners
df = df.dropna().reset_index(drop=True)  # works... until it doesn't

# Step 4: Train model (default linear regression)
X = df[["feature1", "feature2"]]
y = df["target"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = LinearRegression()
model.fit(X_train, y_train)

# Step 5: Save model with pickle (because why not)
with open("model.pkl", "wb") as f:
    pickle.dump(model, f)

# Step 6: Deploy it via SCP (hardcoded credentials? you bet)
os.system("scp model.pkl prod-server:/var/www/html/models/model.pkl")

print("Model deployed to production. Nothing could possibly go wrong.")
\end{lstlisting}


It pulled packages from four different ecosystems, redefined variables mid-loop, and had logging statements that doubled as performance metrics. It learned, it inferred, it emailed the results to itself at 3am.

There were no functions... only faith.  
Tehre was no error handling... only optimism.

The monolith had been born.

It was ugly. It was fragile. It was unreadable to everyone, including the intern who wrote it.

But—miraculously—it worked.

\begin{quote}
And for one brief, cursed moment, the system was in balance.
\end{quote}



\vspace{1em}
\subsection{Act III: The Fall}

Weeks passed.

No one touched the script. No one asked about the model.

It sat in its digital corner, whirring quietly inside a Docker container no one dared to rebuild, delivering predictions with the eerie consistency of a haunted Roomba—faithful, silent, and faintly menacing.

Until, of course, the day it didn’t.

It was a Friday afternoon. The kind of Friday afternoon when half the team had already mentally clocked out and the other half was pretending to write documentation.

Then it happened.

\textbf{The logs changed.}

Not a dramatic change—just enough to be invisible to humans, but fatal to regex. A new column had quietly appeared, unnamed and smug. One old column, long relied upon, was gone without a trace. The date format had subtly shifted—no longer slashes, but dashes. Or was it the timezone? No one knew. The changes crept in like code gremlins, tiptoeing past CI pipelines.

And the script—poor, unsuspecting monolith that it was—choked.

The model broke.

The service failed.

Metrics flatlined. Dashboards turned a shade redder than anyone was comfortable with.

PagerDuty screamed into the void.

On-call phones vibrated on desks and nightstands with all the fury of a thousand missed deadlines. Slack channels lit up like a Christmas tree in a lightning storm. Messages began with \texttt{"Hey..."} and ended with \texttt{"@here."}

The intern, who had last touched the code three rotations ago, was summoned like a wizard in exile. Somewhere, their laptop opened slowly.

A senior SRE, already running on two Red Bulls and a personal grudge against YAML, was seen storming through Grafana panels, muttering ancient incantations like \texttt{"rollback,"} \texttt{"rollback now,"} and \texttt{"who approved this?"}

The autopsy hadn’t started, but the blame game had.

And deep in a log folder, timestamped exactly 15 minutes before failure, the script had left one final message:

\texttt{INFO | Prediction succeeded. target=4.0}

Its last words.

\begin{lstlisting}[caption={Slack transcript, 2:43 PM on a Friday}, label={lst:slackpanic}, basicstyle=\ttfamily\small, frame=single]
#sre-oncall

[2:43 PM] @alertbot: [ALERT] PROD Model Service Failure - HTTP 500s detected
[2:44 PM] @senior_sre: who owns this??
[2:44 PM] @eng_manager: wasn't this the intern's thing? 
[2:45 PM] @intern: hi yes uh give me 5 min
[2:46 PM] @senior_sre: what changed?
[2:46 PM] @intern: the logs have... evolved?
[2:47 PM] @intern: feature2 is now feature3
[2:47 PM] @intern: also the timestamps are in ISO format? I think?
[2:48 PM] @senior_sre: where's the source? 
[2:48 PM] @intern: it is not in Git
[2:48 PM] @senior_sre:  what
[2:49 PM] @intern: I think I SCPed it from my VM last summer?
[2:49 PM] @eng_manager: do we have any docs?
[2:49 PM] @intern: there was a notebook... on my old laptop.
[2:50 PM] @alertbot: [ALERT] PROD Model Service Crash Loop Restarting 
[2:51 PM] @senior_sre: this is why we don't YOLO deploy models 
[2:51 PM] @intern: understood
[2:52 PM] @eng_manager: action item: rewrite everything
\end{lstlisting}


They searched for the source code.

Desperation mounting, they combed through old repos, abandoned branches, shared drives with names like \texttt{archive\_final\_bkp\_DO\_NOT\_DELETE}. They unzipped tarballs nested within tarballs like digital matryoshka dolls. They even searched Confluence.

They found nothing.

No Git history. No documentation. Not even a rogue Jupyter notebook half-filled with markdown and regret.

Just a single, mysterious artifact sitting in a forgotten S3 bucket, untouched for months:  
\texttt{final\_model\_v2\_new\_new.pkl}

The filename inspired no confidence. It had clearly been renamed at least three times. Possibly by committee.

No one knew how it had been trained.  
No one knew what data it had seen.  
No one knew what “new\_new” meant.

No random seed. No environment file. No config. Just a pickle file — full of secrets.

The only logs available were the ones that had just broken.

They hadn’t deployed a model.  
They had released an unsupervised cryptid into production.

An eldritch pipeline, stitched together by interns long since graduated, silently shaping the fate of a product roadmap.

And now it was angry.

\begin{lstlisting}[caption={The log that broke the build}, label={lst:falllogs}, basicstyle=\ttfamily\small, frame=single]
2020-01-03 10:15:36 INFO  | feature1=13.0 feature2=8.1 target=4.1

% One week later...
2020-01-10 10:15:36 INFO  | feature1=13.0 feature3=foo status=OK timestamp=2020-01-10T10:15:36Z
\end{lstlisting}

\subsection{Act IV: The Redemption}

Eventually, the ghost was found.

Not quickly, and not cleanly, but found---buried in an old user directory, hidden behind a filename that looked like keyboard spam and timestamped in a timezone no one could identify.

The model was reverse-engineered.

The service was patched.

A new script---slightly more modular, slightly more commented---was written under duress.

\begin{lstlisting}[caption={The patched script, written under duress}, label={lst:patchscript}, basicstyle=\ttfamily\small, frame=single]
# semi_structured_pipeline.py

import os
import re
import pickle
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

LOG_PATH = "/mnt/data_logs/new_logs_2020b/"  # not hardcoded, just... less hardcoded
MODEL_OUTPUT = "final_model_v3_recovery.pkl"

def parse_log_line(line):
    match = re.match(
        r".*INFO\s+\|\s+feature1=(\d+\.\d+)\s+(?:feature2|feature3)=(\d+\.\d+)\s+target=(\d+\.\d+)",
        line
    )
    return tuple(map(float, match.groups())) if match else None

def load_data_from_logs(log_path):
    entries = []
    for filename in os.listdir(log_path):
        if filename.endswith(".log"):
            with open(os.path.join(log_path, filename), "r") as file:
                for line in file:
                    parsed = parse_log_line(line)
                    if parsed:
                        entries.append(parsed)
    return pd.DataFrame(entries, columns=["feature1", "featureX", "target"])

def train_model(df):
    df = df.dropna().reset_index(drop=True)
    X = df[["feature1", "featureX"]]
    y = df["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    model = LinearRegression()
    model.fit(X_train, y_train)
    return model

def save_model(model, path):
    with open(path, "wb") as f:
        pickle.dump(model, f)

def main():
    print("[INFO] Recovery Mode Initiated")
    df = load_data_from_logs(LOG_PATH)
    if df.empty:
        print("[ERROR] No usable data found. Aborting.")
        return
    model = train_model(df)
    save_model(model, MODEL_OUTPUT)
    print("[INFO] Model saved to", MODEL_OUTPUT)

if __name__ == "__main__":
    main()
\end{lstlisting}


Postmortems were held.

Slides were made.

The phrase “we should really build a pipeline for this” was said no fewer than five times, then quietly ignored.

The intern lived to tell the tale.

And, somehow, was offered a return offer.

Because deep down, everyone knew the truth:

\begin{quote}
The problem wasn’t the intern.  It was the system that let a one-off script become infrastructure.  (Also, the intern was the only one who remembered how \texttt{v2FlagTemp} was derived. So they kind of had to.)
\end{quote}

\subsection{Act V: The Pipeline That Could Have Been}

In a better world—one with deadlines that flex and teams that plan—this story could’ve gone very differently.

Instead of a one-shot pickle file hurled into prod like a cursed artifact, they could have built a proper pipeline.

Not a monolith. Not a notebook duct-taped into a cron job. A real, honest-to-goodness data pipeline. One with stages. One with structure. One that didn’t cause PagerDuty to cry in the middle of a sprint demo.

\textbf{Enter: Airflow.}

With Airflow, each step of the intern’s unholy monolith could’ve been a task in a Directed Acyclic Graph (DAG). Each task could’ve had retries. Timeouts. Logging. Alerts that didn’t rely on Slack sleuthing.

\begin{itemize}
  \item A task for loading logs (with schema checks, no less!).
  \item A task for parsing and validating those logs (with actual error handling!).
  \item A task for cleaning and transforming the data (instead of blindly \texttt{dropna()}-ing your sins away).
  \item A task for training the model (with versioning, reproducibility, and metrics).
  \item And a final task for deployment (ideally, not with \texttt{scp}).
\end{itemize}

Each step modular. Each failure traceable. Each retry logged, monitored, and contained.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
      node distance=1.1cm and 1.8cm,
      dagnode/.style={
        draw, rounded corners, minimum width=2.4cm, minimum height=0.8cm,
        font=\scriptsize, align=center, fill=blue!5
      },
      errnode/.style={
        draw, dashed, minimum width=2.4cm, minimum height=0.8cm,
        font=\scriptsize, align=center, fill=red!10
      },
      groupbox/.style={
        draw, rounded corners, thick, inner sep=0.5em, fill=blue!2!white
      },
      arrow/.style={->, thick}
    ]
    
    % Main flow nodes
    \node[dagnode, fill=gray!10] (start) {Start};
    \node[dagnode, below=of start] (load) {Load Logs};
    \node[dagnode, below=of load] (validate) {Validate Schema};
    \node[dagnode, below=of validate] (parse) {Parse Logs};
    \node[dagnode, below=of parse] (clean) {Clean Data};
    \node[dagnode, below=of clean] (train) {Train Model};
    \node[dagnode, below=of train] (evaluate) {Evaluate};
    \node[dagnode, below=of evaluate] (deploy) {Deploy Model};
    \node[dagnode, fill=gray!10, below=of deploy] (end) {End};
    
    % Error handler node
    \node[errnode, right=4.5cm of clean] (error) {Handle Error\\ (TriggerRule.ONE\_FAILED)};
    
    % Group box in the background layer
    \begin{pgfonlayer}{background}
      \node[groupbox, fit=(load)(validate)(parse)(clean)(train)(evaluate)(deploy), label={[align=center]above:\textbf{Execution Block}}] (taskgroup) {};
    \end{pgfonlayer}
    
    % Main path arrows
    \draw[arrow] (start) -- (load);
    \draw[arrow] (load) -- (validate);
    \draw[arrow] (validate) -- (parse);
    \draw[arrow] (parse) -- (clean);
    \draw[arrow] (clean) -- (train);
    \draw[arrow] (train) -- (evaluate);
    \draw[arrow] (evaluate) -- (deploy);
    \draw[arrow] (deploy) -- (end);
    
    % Shared error handler arrow
    \draw[arrow, dashed] (taskgroup.east) -- (error.west);
    
    \end{tikzpicture}
    \caption{Airflow DAG with a linear execution block and a shared error handler.}
    \label{fig:airflowdag_grouped}
\end{figure}
    
    
    


Instead of a haunted Roomba running in silence, it could’ve been a well-lit assembly line with guardrails, dashboards, and dignity.

Better yet, Airflow could’ve scheduled the job, rather than depending on a Bash script tied to someone’s user crontab. If the logs changed? The schema check would’ve failed. The DAG would’ve paused. Someone would’ve been notified before the VP’s dashboard went full red alert.

And most importantly—every DAG run would’ve left breadcrumbs: metadata, timestamps, artifact hashes. Something future interns could follow without needing a séance.

\begin{itemize}
    \item \textbf{Functions?} Written once, reused across tasks.  
    \item \textbf{Secrets?} Stored in a vault, not hardcoded in a script named \texttt{lolmodel.py}.  
    \item \textbf{Logs?} Structured, queryable, archived.  
    \item \textbf{Documentation?} Okay, maybe that’s still wishful thinking. But everything else? Achievable.
\end{itemize}

Sure, it might’ve taken an extra week to set up.  Sure, it wouldn’t fit in a single slide.  But it would’ve survived the reorg.  It would’ve survived Friday.

And no one would’ve had to grep for salvation.


\begin{lstlisting}[caption={Load logs from a directory.}, label={lst:load_logs}]
    def load_logs(log_dir: str) -> List[str]:
        import os
        log_lines = []
        for filename in os.listdir(log_dir):
            if filename.endswith(".log"):
                with open(os.path.join(log_dir, filename), "r") as f:
                    log_lines.extend(f.readlines())
        if not log_lines:
            raise FileNotFoundError("No logs found in directory.")
        return log_lines
\end{lstlisting}
    

\begin{lstlisting}[caption={Parse structured fields from log lines.}, label={lst:parse_logs}]
    def parse_logs(lines: List[str]) -> List[Tuple[float, float, float]]:
        import re
        parsed = []
        for line in lines:
            match = re.search(r"feature1=(\d+\.\d+).*?(feature2|feature3)=(\d+\.\d+).*?target=(\d+\.\d+)", line)
            if match:
                f1 = float(match.group(1))
                fx = float(match.group(3))
                target = float(match.group(4))
                parsed.append((f1, fx, target))
            else:
                print(f"[WARN] Failed to parse line: {line.strip()}")
        return parsed
\end{lstlisting}



\begin{lstlisting}[caption={Clean and standardize the parsed DataFrame.}, label={lst:clean_data}]
    def clean_data(df: pd.DataFrame) -> pd.DataFrame:
        df = df.rename(columns={"feature2": "featureX", "feature3": "featureX"})
        df = df.dropna().reset_index(drop=True)
        if df.empty:
            raise ValueError("No clean data left after dropping nulls.")
        return df
\end{lstlisting}



\begin{lstlisting}[caption={Train a linear model using scikit-learn.}, label={lst:train_model}]
    def train_model(df: pd.DataFrame) -> LinearRegression:
        from sklearn.linear_model import LinearRegression
        from sklearn.model_selection import train_test_split
    
        X = df[["feature1", "featureX"]]
        y = df["target"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
        model = LinearRegression()
        model.fit(X_train, y_train)
        return model
\end{lstlisting}


\begin{lstlisting}[caption={Deploy the trained model by pickling it.}, label={lst:deploy_model}]
    def deploy_model(model, output_path="final_model_v4.pkl"):
        import pickle
        with open(output_path, "wb") as f:
            pickle.dump(model, f)
        print(f"[INFO] Model saved to {output_path}")
        # Simulate deployment
        # subprocess.run(["scp", output_path, "prod-server:/var/www/html/models/"])
\end{lstlisting}


\begin{lstlisting}[caption={Airflow DAG with linear tasks and error handling flow.}, label={lst:airflowdag_error_handling}]
    from airflow.decorators import dag, task
    from airflow.operators.empty import EmptyOperator
    from airflow.operators.python import PythonOperator
    from airflow.utils.trigger_rule import TriggerRule
    from datetime import datetime
    
    @dag(schedule_interval="@daily", start_date=datetime(2023, 1, 1), catchup=False, tags=["modular_pipeline"])
    def resilient_model_pipeline():
    
        start = EmptyOperator(task_id="start")
        end = EmptyOperator(task_id="end")
    
        # Fallback error handler task
        error_handler = PythonOperator(
            task_id="handle_error",
            python_callable=lambda: print("[ERROR] Pipeline failure detected. Escalating."),
            trigger_rule=TriggerRule.ONE_FAILED
        )
    
        @task()
        def load_logs_task():
            return load_logs("/mnt/logs")
    
        @task()
        def validate_schema_task(logs):
            if not logs:
                raise ValueError("Log validation failed: Empty or missing.")
            return logs
    
        @task()
        def parse_logs_task(validated_logs):
            return parse_logs(validated_logs)
    
        @task()
        def clean_data_task(parsed):
            import pandas as pd
            df = pd.DataFrame(parsed, columns=["feature1", "featureX", "target"])
            return clean_data(df)
    
        @task()
        def train_model_task(df):
            return train_model(df)
    
        @task()
        def evaluate_model(model):
            print("[INFO] Evaluating model...")
            return "evaluation complete"
    
        @task()
        def deploy_model_task(eval_result, model):
            deploy_model(model)
    
        # DAG wiring
        logs = load_logs_task()
        validated = validate_schema_task(logs)
        parsed = parse_logs_task(validated)
        cleaned = clean_data_task(parsed)
        model = train_model_task(cleaned)
        eval_result = evaluate_model(model)
        deploy = deploy_model_task(eval_result, model)
    
        # Dependencies
        start >> logs >> validated >> parsed >> cleaned >> model >> eval_result >> deploy >> end
    
        # Attach error handler to all main tasks (except start/end)
        for t in [logs, validated, parsed, cleaned, model, eval_result, deploy]:
            t >> error_handler
    
    dag = resilient_model_pipeline()
    \end{lstlisting}





\subsection{version 2}



\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
      node distance=1.1cm and 1.8cm,
      dagnode/.style={
        draw, rounded corners, minimum width=2.6cm, minimum height=0.8cm,
        font=\scriptsize, align=center, fill=blue!5
      },
      errnode/.style={
        draw, dashed, minimum width=2.6cm, minimum height=0.8cm,
        font=\scriptsize, align=center, fill=red!10
      },
      groupbox/.style={
        draw, rounded corners, thick, inner sep=0.5em, fill=blue!2!white
      },
      arrow/.style={->, thick}
    ]
    
    % Main flow
    \node[dagnode, fill=gray!10] (start) {Start};
    \node[dagnode, below=of start] (load) {Load Logs};
    \node[dagnode, below=of load] (validate) {Validate Schema};
    \node[dagnode, below=of validate] (parse) {Parse Logs};
    
    % Parallel tasks
    \node[dagnode, below left=1.2cm and 1.5cm of parse] (quality) {Validate Quality};
    \node[dagnode, below right=1.2cm and 1.5cm of parse] (features) {Feature Engineering};
    
    % Join and continue
    \node[dagnode, below=1.8cm of parse] (train) {Train Model};
    \node[dagnode, below=of train] (evaluate) {Evaluate};
    \node[dagnode, below=of evaluate] (deploy) {Deploy Model};
    \node[dagnode, fill=gray!10, below=of deploy] (end) {End};
    
    % Error handler
    \node[errnode, right=5.3cm of train] (error) {Handle Error\\ (TriggerRule.ONE\_FAILED)};
    
    % Group box in background
    \begin{pgfonlayer}{background}
      \node[groupbox, fit=(load)(validate)(parse)(quality)(features)(train)(evaluate)(deploy), 
            label={[align=center]above:\textbf{Execution Block}}] (taskgroup) {};
    \end{pgfonlayer}
    
    % Main arrows
    \draw[arrow] (start) -- (load);
    \draw[arrow] (load) -- (validate);
    \draw[arrow] (validate) -- (parse);
    \draw[arrow] (parse) -- (quality);
    \draw[arrow] (parse) -- (features);
    \draw[arrow] (quality) -- (train);
    \draw[arrow] (features) -- (train);
    \draw[arrow] (train) -- (evaluate);
    \draw[arrow] (evaluate) -- (deploy);
    \draw[arrow] (deploy) -- (end);
    
    % Error path from group
    \draw[arrow, dashed] (taskgroup.east) -- (error.west);
    
    \end{tikzpicture}
    \caption{Airflow DAG with parallel validation and feature engineering inside the execution block, and a shared error handler.}
    \label{fig:airflowdag_parallel}
\end{figure}




\begin{lstlisting}[caption={Airflow DAG with parallel data quality validation and feature engineering before training.}, label={lst:airflowdag_parallel}]
    from airflow.decorators import dag, task
    from airflow.operators.empty import EmptyOperator
    from airflow.operators.python import PythonOperator
    from airflow.utils.trigger_rule import TriggerRule
    from datetime import datetime
    
    @dag(schedule_interval="@daily", start_date=datetime(2023, 1, 1), catchup=False, tags=["modular_pipeline"])
    def enhanced_pipeline():
    
        start = EmptyOperator(task_id="start")
        end = EmptyOperator(task_id="end")
    
        # Shared error handler
        error_handler = PythonOperator(
            task_id="handle_error",
            python_callable=lambda: print("[ERROR] Pipeline failure detected."),
            trigger_rule=TriggerRule.ONE_FAILED
        )
    
        @task()
        def load_logs_task():
            return load_logs("/mnt/logs")
    
        @task()
        def validate_schema_task(logs):
            if not logs:
                raise ValueError("Log validation failed: Empty or missing.")
            return logs
    
        @task()
        def parse_logs_task(validated_logs):
            return parse_logs(validated_logs)
    
        @task()
        def validate_quality_task(parsed_data):
            # e.g., check missing values, outliers, etc.
            print("Validating data quality...")
            return parsed_data
    
        @task()
        def generate_features_task(parsed_data):
            # e.g., feature transformation, normalization
            print("Engineering features...")
            return parsed_data
    
        @task()
        def train_model_task(inputs1, inputs2):
            import pandas as pd
            # Merge cleaned inputs
            df = pd.DataFrame(inputs1, columns=["feature1", "featureX", "target"])
            return train_model(df)
    
        @task()
        def evaluate_model(model):
            print("[INFO] Evaluating model...")
            return "evaluation complete"
    
        @task()
        def deploy_model_task(eval_result, model):
            deploy_model(model)
    
        # Wiring
        logs = load_logs_task()
        validated = validate_schema_task(logs)
        parsed = parse_logs_task(validated)
    
        quality = validate_quality_task(parsed)
        features = generate_features_task(parsed)
    
        model = train_model_task(quality, features)
        eval_result = evaluate_model(model)
        deploy = deploy_model_task(eval_result, model)
    
        start >> logs >> validated >> parsed
        parsed >> [quality, features]
        [quality, features] >> model >> eval_result >> deploy >> end
    
        for t in [logs, validated, parsed, quality, features, model, eval_result, deploy]:
            t >> error_handler
    
    dag = enhanced_pipeline()
\end{lstlisting}

    


\subsection{version 3}


\begin{lstlisting}[caption={Airflow DAG with parallel data quality validation and feature engineering before training.}, label={lst:airflowdag_parallel}]
from airflow.decorators import dag
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.utils.trigger_rule import TriggerRule
from datetime import datetime

from tasks.load_logs import load_logs_task
from tasks.validate_schema import validate_schema_task
from tasks.parse_logs import parse_logs_task
from tasks.validate_quality import validate_quality_task
from tasks.generate_features import generate_features_task
from tasks.train_model import train_model_task
from tasks.evaluate_model import evaluate_model_task
from tasks.deploy_model import deploy_model_task

@dag(schedule_interval="@daily", start_date=datetime(2023, 1, 1), catchup=False, tags=["modular_pipeline"])
def enhanced_pipeline():
    start = EmptyOperator(task_id="start")
    end = EmptyOperator(task_id="end")

    error_handler = PythonOperator(
        task_id="handle_error",
        python_callable=lambda: print("[ERROR] Pipeline failure detected."),
        trigger_rule=TriggerRule.ONE_FAILED
    )

    logs = load_logs_task()
    validated = validate_schema_task(logs)
    parsed = parse_logs_task(validated)

    quality = validate_quality_task(parsed)
    features = generate_features_task(parsed)

    model = train_model_task(quality, features)
    eval_result = evaluate_model_task(model)
    deploy = deploy_model_task(eval_result, model)

    start >> logs >> validated >> parsed
    parsed >> [quality, features]
    [quality, features] >> model >> eval_result >> deploy >> end

    for t in [logs, validated, parsed, quality, features, model, eval_result, deploy]:
        t >> error_handler

dag = enhanced_pipeline()
\end{lstlisting}




\lstdefinestyle{tree}{
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{gray!5},
  frame=single,
  columns=fullflexible,
  showstringspaces=false
}

\begin{lstlisting}[style=tree, caption={Modular DAG file structure (ASCII-safe)}, label={lst:dag_tree_ascii}]
    dags/
    |- enhanced_pipeline_dag.py         # DAG definition & task wiring
    |- tasks/
       |- __init__.py
       |- load_logs.py
       |- validate_schema.py
       |- parse_logs.py
       |- validate_quality.py
       |- generate_features.py
       |- train_model.py
       |- evaluate_model.py
       |- deploy_model.py
\end{lstlisting}



\subsection{version 4}

\begin{lstlisting}[style=tree, caption={Modular DAG file structure (ASCII-safe)}, label={lst:dag_tree_ascii}]
    dags/
    |- enhanced_pipeline/
    |  |- __init__.py
    |  |- enhanced_pipeline_dag.py         # DAG definition + @dag wrapper
    |  |- task_factory.py                  # contains create_tasks()
    |  |- task_wiring.py                   # contains wire_tasks()
    |  |- orchestrator.py                  # contains orchestrate_dag()
    |
    |- tasks/
    |  |- __init__.py
    |  |- load_logs.py
    |  |- validate_schema.py
    |  |- parse_logs.py
    |  |- validate_quality.py
    |  |- generate_features.py
    |  |- train_model.py
    |  |- evaluate_model.py
    |  |- deploy_model.py
    |
    |- utils/
       |- __init__.py
       |- io.py                            # e.g., load_logs()
       |- ml.py                            # e.g., train_model(), etc.
    
\end{lstlisting}

\begin{lstlisting}[caption={Airflow DAG with parallel data quality validation and feature engineering before training.}, label={lst:airflowdag_parallel}]
    from airflow.decorators import dag
    from airflow.operators.empty import EmptyOperator
    from airflow.operators.python import PythonOperator
    from airflow.utils.trigger_rule import TriggerRule
    from datetime import datetime
    
    from tasks.load_logs import load_logs_task
    from tasks.validate_schema import validate_schema_task
    from tasks.parse_logs import parse_logs_task
    from tasks.validate_quality import validate_quality_task
    from tasks.generate_features import generate_features_task
    from tasks.train_model import train_model_task
    from tasks.evaluate_model import evaluate_model_task
    from tasks.deploy_model import deploy_model_task
    
    
    def create_tasks():
        """Return a dictionary of all tasks with dependencies left unwired."""
        start = EmptyOperator(task_id="start")
        end = EmptyOperator(task_id="end")
    
        error_handler = PythonOperator(
            task_id="handle_error",
            python_callable=lambda: print("[ERROR] Pipeline failure detected."),
            trigger_rule=TriggerRule.ONE_FAILED
        )
    
        logs = load_logs_task()
        validated = validate_schema_task(logs)
        parsed = parse_logs_task(validated)
    
        quality = validate_quality_task(parsed)
        features = generate_features_task(parsed)
    
        model = train_model_task(quality, features)
        eval_result = evaluate_model_task(model)
        deploy = deploy_model_task(eval_result, model)
    
        return {
            "start": start,
            "end": end,
            "error_handler": error_handler,
            "logs": logs,
            "validated": validated,
            "parsed": parsed,
            "quality": quality,
            "features": features,
            "model": model,
            "eval_result": eval_result,
            "deploy": deploy
        }
    
    
    def wire_tasks(t):
        """Connect tasks in the desired DAG structure."""
        t["start"] >> t["logs"] >> t["validated"] >> t["parsed"]
        t["parsed"] >> [t["quality"], t["features"]]
        [t["quality"], t["features"]] >> t["model"] >> t["eval_result"] >> t["deploy"] >> t["end"]
    
        for task in [
            t["logs"], t["validated"], t["parsed"],
            t["quality"], t["features"],
            t["model"], t["eval_result"], t["deploy"]
        ]:
            task >> t["error_handler"]
    
    
    def orchestrate_dag():
        """High-level orchestration of task creation and wiring."""
        tasks = create_tasks()
        wire_tasks(tasks)
    
    
    @dag(schedule_interval="@daily", start_date=datetime(2023, 1, 1), catchup=False, tags=["modular_pipeline"])
    def enhanced_pipeline():
        orchestrate_dag()
    
    
dag = enhanced_pipeline()
\end{lstlisting}
    
\begin{table}[H]
    \centering
    \begin{tabular}{|l|p{10cm}|}
    \hline
    \textbf{File} & \textbf{Responsibility} \\
    \hline
    \texttt{enhanced\_pipeline\_dag.py} & Contains \texttt{@dag} decorator and calls \texttt{orchestrate\_dag()} \\
    \hline
    \texttt{task\_factory.py} & Defines and returns all task objects (\texttt{create\_tasks()}) \\
    \hline
    \texttt{task\_wiring.py} & Wires the task dependencies (\texttt{wire\_tasks(tasks)}) \\
    \hline
    \texttt{orchestrator.py} & Calls both functions (\texttt{orchestrate\_dag()}) \\
    \hline
    \texttt{tasks/*.py} & Each contains a single \texttt{@task} Airflow function \\
    \hline
    \texttt{utils/io.py} & Low-level data access helpers (e.g., log loading) \\
    \hline
    \texttt{utils/ml.py} & Model logic, feature engineering, and evaluation helpers \\
    \hline
    \end{tabular}
    \caption{Modular DAG code responsibilities by file}
    \label{tab:dag_structure}
\end{table}
    

    





\subsection{Epilogue: Why Modularity Matters}

Let’s be honest.

This story didn’t end in disaster because the intern was reckless.  It ended in disaster because the system had no guardrails.

When a one-off training script, duct-taped together with assumptions, globals, and good intentions, is all it takes to deploy to production, the issue isn’t code quality---it’s architecture.

Modularity isn’t just a buzzword consultants put in slide decks.  It’s a survival strategy.

If the ingestion logic had been its own component---versioned, tested, isolated---a format change wouldn’t have brought down the system.\\
If training and deployment had been separate stages with inputs, outputs, and contracts, we wouldn’t have been staring at a mystery \texttt{.pkl} file like it was an alien artifact.

Modularity gives you reproducibility.  It gives you traceability. It lets you swap a broken piece without disassembling the entire machine.

Without it?

You get fragile scripts, ghost models, and institutional knowledge stored in a single intern’s memory.

Which is fine\ldots until they graduate.


\vspace{1em}
\textbf{A Visual, in Your Mind}

Picture a Jenga tower.

A modular system lets you pull out one block and put in a better one.\\
A monolithic system? One wrong move, and the whole thing comes crashing down.

Your move.

\subsection{Case Study Revisited: The Intern and the Forgotten Monolith}

Imagine if this methodology had been applied before the intern ever touched \texttt{pandas.read\_csv()}.

A mindful diagnostic session might have revealed:
\begin{itemize}
    \item The system was already exhibiting **Shadow** behaviors (ancient scripts no one understood).
    \item The ownership was ambiguous, signaling a Conway-induced **Orphan**.
    \item The culture prioritized speed over structure—a breeding ground for Hero-driven quick fixes.
\end{itemize}

Instead of celebrating the intern’s short-term “success,” the team could have paused, recognized these patterns, and intervened—not with a heavy governance hand, but with lightweight guardrails:

\begin{itemize}
    \item Mandate modularity not as a rule, but as a reflective practice.
    \item Document the why, not just the what.
    \item Treat every ad-hoc script as a potential **patient zero** for future dysfunction.
\end{itemize}






