\section{Noether's Theorem and the Dynamics of Energy in Information Systems}



\subsection{Noether's Theorem and the Meaning of Power}

In the previous sections, we explored how different transformations—Fourier, Z-transform, wavelets—reveal different structural features of a signal. Each yields a different \textbf{power spectrum}, showing how a signal’s energy is distributed across modes, frequencies, or recursive patterns.

But what exactly is being conserved in this power? What is this energy, and why does it stay invariant across these transformations?

To answer that, we need to go back to the Lagrangian formulation of physics—and to \textbf{Noether’s Theorem}.

\vspace{1em}
\noindent
In physics, a system’s evolution can be described by a Lagrangian function \( L(q, \dot{q}, t) \), which encodes the difference between kinetic and potential energy. If this Lagrangian is invariant under certain transformations—such as shifts in time or space—then a corresponding physical quantity is conserved.

\begin{quote}
\textbf{Noether’s Theorem:} \emph{Every continuous symmetry of the action leads to a conserved quantity.}
\end{quote}

In mechanics:
\begin{itemize}
    \item Time translation symmetry \( \Rightarrow \) Conservation of energy
    \item Spatial translation symmetry \( \Rightarrow \) Conservation of linear momentum
    \item Rotational symmetry \( \Rightarrow \) Conservation of angular momentum
\end{itemize}

So what does this have to do with signals?

When we analyze signals using tools like the Fourier transform, we are implicitly assuming that the system (or signal) is \emph{stationary}—that it behaves the same over time. This is a form of time-translation symmetry. And just as in mechanics, this symmetry gives rise to a conserved quantity: \textbf{power} in the frequency domain.

\begin{itemize}
    \item In the time domain: \( E = \int |f(t)|^2 dt \)
    \item In the frequency domain: \( E = \int |F(\omega)|^2 d\omega \)
\end{itemize}

This equivalence is known as \textbf{Parseval’s Theorem}, and it’s not just a numerical curiosity—it’s a manifestation of Noether’s principle.

\vspace{1em}
\noindent
\textbf{Why this matters for functional analysis:}

When we apply transformations to functions—Fourier, wavelets, Z-transforms—we're not just re-expressing them. We're mapping them into new function spaces, often Hilbert spaces, where notions like inner products, norms, and energy become precise.

In these spaces:
\begin{itemize}
    \item The \textbf{norm} \( \|f\|^2 = \langle f, f \rangle = \int |f(t)|^2 dt \) becomes the total energy.
    \item A unitary transform \( T \) preserves this norm: \( \|Tf\| = \|f\| \)
    \item So if \( f(t) \) is our signal, and \( Tf \) is its transform (e.g., the Fourier transform), the power spectrum \( |Tf|^2 \) redistributes—but the total remains conserved.
\end{itemize}

This is the core idea: \textbf{power in signal processing is energy in functional analysis}, and it’s conserved because of symmetry—just as Noether told us.

\begin{quote}
In physics, energy is conserved when time is uniform.\\
In signal analysis, power is conserved when structure is uniform.\\
In functional analysis, norms are conserved when transforms respect symmetry.
\end{quote}

Noether’s Theorem is not just about particles—it’s about transformations that preserve meaning. When you apply a Fourier transform to a signal, you’re invoking a symmetry. The power spectrum isn’t just a breakdown of energy—it’s the signature of an invariant.

That’s why different transforms yield different power spectra: each reveals different symmetries and different conserved structures. And when there is no symmetry—when the function is irregular, like the Dirichlet function—we see the breakdown of conservation. The spectrum becomes chaotic. Energy no longer clusters. The function spreads its uncertainty everywhere.

\vspace{1em}
\noindent
\textbf{In summary:}

\begin{itemize}
    \item The “power” in the power spectrum is conserved energy.
    \item Its conservation arises from the symmetries of the signal.
    \item Noether’s Theorem explains why unitary transforms preserve this power.
    \item Functional analysis gives us the framework to measure, decompose, and interpret it.
\end{itemize}

Noether bridged symmetry and conservation. Shannon bridged uncertainty and information. Together, they explain why the decompositions we use in signal processing reveal not just structure, but \emph{meaning}.


\subsection{Power Spectra as Conserved Quantities}

Consider a continuous-time signal \( f(t) \). Its total energy is:

\[
E = \int_{-\infty}^{\infty} |f(t)|^2 \, dt
\]

When we apply the Fourier transform, this energy is redistributed across frequencies, but the total remains invariant:

\[
E = \int_{-\infty}^{\infty} |F(\omega)|^2 \, d\omega
\]

This is Parseval’s Theorem—a signal-theoretic version of Noether’s Theorem. The invariance under time translation (i.e., the physics of the system doesn't change over time) ensures that energy is conserved under transformation.

This remains true whether we use:
\begin{itemize}
    \item The \textbf{Fourier Transform}, emphasizing global periodic structure
    \item The \textbf{Z-Transform}, capturing recursive relationships and memory
    \item \textbf{Wavelet Transforms}, revealing localized structure in time and scale
\end{itemize}

Each gives a different \textit{distribution} of energy across modes or coefficients. But if the transform respects the system's symmetries (i.e., it's unitary or energy-preserving), then the \textbf{total power stays constant}. This is Noether's legacy in signal space.

\subsection{Entropy as Energy’s Twin}

Now recall: entropy is a measure of uncertainty, not energy. But in systems where probability distributions are tied to signal amplitudes (e.g., power spectra interpreted as probability mass functions), entropy becomes a shadow dynamic alongside energy.

\begin{itemize}
    \item A \textbf{narrow} power spectrum (e.g., single dominant frequency) means low entropy—predictable, compressible signal.
    \item A \textbf{broad} power spectrum (e.g., many equal frequencies) means high entropy—uncertain, complex, possibly noisy signal.
\end{itemize}

While Noether’s Theorem tells us energy is conserved across symmetrical transformations, entropy tells us how that energy is \emph{spread}. In systems with no symmetry (e.g., noisy, disordered signals), entropy is high and mutual information is low. In systems with structure, entropy compresses and information flows more cleanly.

\subsection{The Symmetry–Entropy Duality}

This leads us to a deep principle:

\begin{quote}
\textbf{Symmetry conserves energy. Asymmetry spreads entropy.}
\end{quote}

When a system possesses symmetry, conserved quantities flow through transformations untouched. But as symmetry breaks—through noise, non-stationarity, or chaos—entropy increases, and predictability declines.

In this sense, Noether’s Theorem explains more than just motion—it underpins the entire logic of how systems evolve, conserve, and encode information.

\begin{quote}
In mechanics, energy is conserved because time is uniform. \\
In signals, power is conserved because the domain is translationally invariant. \\
In information, structure is conserved when symmetry protects it. \\
Entropy begins when symmetry ends.
\end{quote}
